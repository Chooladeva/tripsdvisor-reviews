{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "To gain deeper insights into customer feedback, it is essential to understand not only the sentiments expressed in hotel reviews but also the specific aspects of the hotel that these sentiments target. This task addresses this by employing topic modelling to cluster reviews into meaningful groups representing distinct hotel aspects such as cleanliness, service, location, and amenities.\n",
    "\n",
    "Since the dataset lacks predefined aspect labels, I approximate ground truth by randomly selecting 50 reviews from the test set, manually labelling their aspects, and comparing them against the clusters produced by the topic model. This validation step ensures that the clusters align closely with human judgment and capture real-world aspects accurately.\n",
    "\n",
    "Finally, the derived clusters are used as aspect labels to implement an aspect-based sentiment classifier, enabling sentiment analysis at the aspect level rather than the entire review level. This approach provides more granular and actionable insights for hotel management and research purposes.\n",
    "\n",
    "---\n",
    "\n",
    "### Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T12:42:14.016033Z",
     "iopub.status.busy": "2025-08-23T12:42:14.015765Z",
     "iopub.status.idle": "2025-08-23T12:42:31.064710Z",
     "shell.execute_reply": "2025-08-23T12:42:31.063920Z",
     "shell.execute_reply.started": "2025-08-23T12:42:14.015967Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 12:42:21.180333: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755952941.203016     199 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755952941.209748     199 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from numpy import mean, zeros\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import scipy.stats as stats\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import adjusted_rand_score, jaccard_score\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 09- Exploring Hotel Service Aspects\n",
    "\n",
    "#### Part A- Topic Modeling and Manual Validation Preparation Using BERTopic\n",
    "\n",
    "BERTopic was selected as the topic modelling approach to identify distinct aspects in hotel reviews because BERTopic combines transformer-based sentence embeddings with advanced clustering algorithms, enabling the extraction of semantically coherent topics from unstructured text. Specifically, the all-MiniLM-L6-v2 sentence transformer was used to generate dense embeddings that capture contextual meaning beyond simple word overlap, allowing reviews with different wording but similar meaning to be grouped together.\n",
    "\n",
    "The following code performs topic modeling on hotel review texts using the BERTopic framework with pre-trained sentence embeddings from the \"all-MiniLM-L6-v2\" model. First, it encodes the cleaned review texts into dense vector embeddings. Then, it applies dimensionality reduction via UMAP to capture the underlying structure of the data, followed by clustering with HDBSCAN to group similar reviews into topics. BERTopic combines these components to assign each review a topic label and calculates the probability of each assignment. It also reduces the impact of outlier topics by reassigning them.\n",
    " \n",
    "**reduce_outliers()**: BERTopic assigns -1 to points that don't clearly belong to any cluster. reduce_outliers() takes those -1 reviews and reassigns them to the most probable cluster, using the learned topic distributions and embeddings. It doesn’t invent new topics, it cleans up the noise and improves cluster coverage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T12:42:31.067293Z",
     "iopub.status.busy": "2025-08-23T12:42:31.066638Z",
     "iopub.status.idle": "2025-08-23T12:42:31.224688Z",
     "shell.execute_reply": "2025-08-23T12:42:31.224134Z",
     "shell.execute_reply.started": "2025-08-23T12:42:31.067271Z"
    }
   },
   "outputs": [],
   "source": [
    "folder_path = '/kaggle/input/sentiment-data'\n",
    "processed_df = pd.read_csv(f\"{folder_path}/processed_sentiment_data.csv\")\n",
    "\n",
    "# Basic cleaning using regex\n",
    "def basic_text_cleaning(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)   # Remove URLs\n",
    "    text = re.sub(r'<.*?>', '', text)                     # Remove HTML\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)                  # Remove non-alphabetic\n",
    "    return text\n",
    "\n",
    "# Apply the function\n",
    "processed_df['basic_text_cleaning'] = processed_df['review_text'].apply(basic_text_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T12:42:31.225649Z",
     "iopub.status.busy": "2025-08-23T12:42:31.225369Z",
     "iopub.status.idle": "2025-08-23T12:44:09.190861Z",
     "shell.execute_reply": "2025-08-23T12:44:09.190042Z",
     "shell.execute_reply.started": "2025-08-23T12:42:31.225622Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2034b974a5a4be8a724deb38be0b8ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load pre-trained sentence transformer model for generating dense vector embeddings\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "# Convert cleaned review texts to a list\n",
    "docs = processed_df['basic_text_cleaning'].tolist()\n",
    "# Generate sentence embeddings for each document\n",
    "embeddings = embedding_model.encode(docs, show_progress_bar=True)\n",
    "\n",
    "# Configure UMAP for dimensionality reduction of embeddings before clustering\n",
    "umap_model = UMAP(n_neighbors=30, min_dist=0.1, metric='cosine', random_state=42)\n",
    "# Set up HDBSCAN clustering algorithm to group similar reviews into topics\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=5, min_samples=1, prediction_data=True)\n",
    "\n",
    "topic_model = BERTopic(embedding_model=embedding_model,\n",
    "                       umap_model=umap_model,\n",
    "                       hdbscan_model=hdbscan_model,\n",
    "                       calculate_probabilities=True)\n",
    "# Initialize BERTopic model with custom embedding, UMAP, and HDBSCAN configurations\n",
    "topics, probs = topic_model.fit_transform(docs, embeddings)\n",
    "# Refine topic assignments by reducing outliers\n",
    "topics = topic_model.reduce_outliers(docs, topics)\n",
    "processed_df['topic'] = topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After topic assignment, the code samples 50 reviews and their corresponding topics for manual validation. It creates a DataFrame with these samples and saves it to a CSV file, facilitating human review to verify and refine the topic labels. Additionally, it reports how many reviews remain unassigned (labeled -1), indicating potential outliers or ambiguous texts. This step helps ensure the quality and interpretability of the topic modeling results in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T12:44:09.192368Z",
     "iopub.status.busy": "2025-08-23T12:44:09.191774Z",
     "iopub.status.idle": "2025-08-23T12:44:09.202680Z",
     "shell.execute_reply": "2025-08-23T12:44:09.201925Z",
     "shell.execute_reply.started": "2025-08-23T12:44:09.192336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unassigned topics (-1): 0\n",
      "Saved manual_label_for_aspect_validation.csv with 50 reviews.\n"
     ]
    }
   ],
   "source": [
    "# Randomly sample 50 rows from the processed dataframe (fixed seed for reproducibility)\n",
    "sampled = processed_df.sample(50, random_state=42)\n",
    "sample_texts = sampled['basic_text_cleaning'].tolist()\n",
    "# Extract the assigned topic labels from the sample as a list\n",
    "sample_topics = sampled['topic'].tolist()\n",
    "\n",
    "\n",
    "# Save for manual labeling\n",
    "manual_check_df = pd.DataFrame({\n",
    "    'Review': sample_texts,\n",
    "    'Assigned_Topic': sample_topics\n",
    "})\n",
    "\n",
    "# Print how many were -1 (unassigned topics)\n",
    "print(\"Number of unassigned topics (-1):\", (manual_check_df['Assigned_Topic'] == -1).sum())\n",
    "\n",
    "manual_check_df.to_csv('manual_label_for_aspect_validation.csv', index=False)\n",
    "print(\"Saved manual_label_for_aspect_validation.csv with 50 reviews.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B- Evaluating Topic Alignment Between Manual and BERTopic Labels\n",
    "\n",
    "This following code evaluates how well BERTopic's automatically assigned topics align with human-annotated aspect labels using clustering evaluation metrics. It starts by loading a CSV file containing 50 manually labeled hotel reviews (e.g., food, Service, room, Loaction) along with their predicted BERTopic topic IDs. The manual and predicted labels are extracted, with manual labels encoded into integers using LabelEncoder for compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T12:44:09.204313Z",
     "iopub.status.busy": "2025-08-23T12:44:09.203441Z",
     "iopub.status.idle": "2025-08-23T12:44:09.227585Z",
     "shell.execute_reply": "2025-08-23T12:44:09.226876Z",
     "shell.execute_reply.started": "2025-08-23T12:44:09.204264Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  Assigned_Topic  \\\n",
      "0  we had a fantastic stay the room was very plea...              24   \n",
      "1  hotel stay was very comfortable food was excel...             117   \n",
      "2  saman villas is an excellent hotel with a uniq...             148   \n",
      "3  what a great stay we booked two rooms foods a ...             303   \n",
      "4  we had chosen the riu at ahungalla for our ann...              93   \n",
      "\n",
      "  Manual_Topic  \n",
      "0      Service  \n",
      "1         Food  \n",
      "2      Service  \n",
      "3         Food  \n",
      "4     Location  \n",
      "Total reviews loaded: 50\n",
      "Columns: ['Review', 'Assigned_Topic', 'Manual_Topic']\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file with the manual labels and assigned BERTopic topics\n",
    "manual_file_path = '/kaggle/input/manual-label-for-aspect-labeled/manual_label_for_aspect_labeled.csv'\n",
    "manual_check = pd.read_csv(manual_file_path)\n",
    "\n",
    "# Clean the manual labels column to avoid duplicates due to casing or whitespace\n",
    "manual_check['Manual_Topic'] = manual_check['Manual_Topic'].str.strip().str.title()\n",
    "\n",
    "# Quick check\n",
    "print(manual_check.head(5))\n",
    "print(f\"Total reviews loaded: {len(manual_check)}\")\n",
    "print(f\"Columns: {manual_check.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the topic numbers assigned by BERTopic don’t necessarily match the human-assigned ones, it applies the Hungarian Algorithm (via linear_sum_assignment) to optimally match predicted topic labels to manual labels based on the confusion matrix. This mapping minimizes the total misalignment between the predicted and actual topics. Then, it computes two clustering evaluation metrics:\n",
    "\n",
    "- **Adjusted Rand Index (ARI):** Measures similarity between two clustering assignments while correcting for chance. A score closer to 1 indicates strong agreement.\n",
    "- **Jaccard Coefficient (Macro-average):** Measures the intersection-over-union of predicted and true labels across all classes, averaged across classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T12:44:09.228558Z",
     "iopub.status.busy": "2025-08-23T12:44:09.228332Z",
     "iopub.status.idle": "2025-08-23T12:44:09.244762Z",
     "shell.execute_reply": "2025-08-23T12:44:09.244031Z",
     "shell.execute_reply.started": "2025-08-23T12:44:09.228532Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Index (aligned): -0.0386\n",
      "Jaccard Coefficient (macro-average, aligned): 0.0101\n"
     ]
    }
   ],
   "source": [
    "manual_labels_raw = manual_check['Manual_Topic'].values\n",
    "predicted_labels_raw = manual_check['Assigned_Topic'].values\n",
    "\n",
    "# Encode labels for metrics\n",
    "le = LabelEncoder()\n",
    "manual_labels = le.fit_transform(manual_labels_raw)\n",
    "predicted_labels = predicted_labels_raw.astype(int)\n",
    "\n",
    "# Hungarian alignment\n",
    "conf_mat = confusion_matrix(manual_labels, predicted_labels)\n",
    "row_ind, col_ind = linear_sum_assignment(-conf_mat)\n",
    "mapping = dict(zip(col_ind, row_ind))\n",
    "mapped_preds = [mapping.get(p, -1) for p in predicted_labels]\n",
    "\n",
    "ari = adjusted_rand_score(manual_labels, mapped_preds)\n",
    "jac = jaccard_score(manual_labels, mapped_preds, average='macro')\n",
    "\n",
    "print(f\"Adjusted Rand Index (aligned): {ari:.4f}\")\n",
    "print(f\"Jaccard Coefficient (macro-average, aligned): {jac:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation:\n",
    "\n",
    "**The Adjusted Rand Index (-0.0386) and Jaccard Coefficient (0.0101) being low means the BERTopic-generated clusters don't align well with the manual aspect labels (\"Room\", \"Service\", \"Food\", \"Location\").**\n",
    "\n",
    "That's expected because:\n",
    "- BERTopic clusters based on semantic similarity, not manually defined aspects.\n",
    "- Aspects like \"Room\", \"Food\", and \"Service\" are human-defined categories, while topic models group linguistic patterns.\n",
    "- Reviews often contain multiple aspects (e.g., \"The room was great but the service was terrible\"). BERTopic assigns one topic per doc.\n",
    "- The manual sample is small (50 reviews), this amplifies noise and reduces alignment with unsupervised clusters.\n",
    "\n",
    "#### Tuning BERTopic with Fixed Manual Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T12:44:09.245686Z",
     "iopub.status.busy": "2025-08-23T12:44:09.245455Z",
     "iopub.status.idle": "2025-08-23T12:44:12.601365Z",
     "shell.execute_reply": "2025-08-23T12:44:12.600634Z",
     "shell.execute_reply.started": "2025-08-23T12:44:09.245670Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7496a8f7e9754bf080d7160c4139e289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "texts = manual_check['Review'].tolist()\n",
    "true_labels = manual_check['Manual_Topic'].tolist()\n",
    "\n",
    "# Encode Text Using Better Embeddings\n",
    "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "embeddings = embedding_model.encode(texts, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T12:44:12.604156Z",
     "iopub.status.busy": "2025-08-23T12:44:12.603884Z",
     "iopub.status.idle": "2025-08-23T12:44:16.585719Z",
     "shell.execute_reply": "2025-08-23T12:44:16.585057Z",
     "shell.execute_reply.started": "2025-08-23T12:44:12.604136Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trying: UMAP min_dist=0.5, HDBSCAN min_cluster_size=10\n",
      "ARI: -0.0290, Jaccard: 0.0783\n",
      "\n",
      "Trying: UMAP min_dist=0.3, HDBSCAN min_cluster_size=15\n",
      "ARI: 0.0000, Jaccard: 0.0500\n",
      "\n",
      "Trying: UMAP min_dist=0.4, HDBSCAN min_cluster_size=12\n",
      "ARI: 0.0565, Jaccard: 0.1062\n",
      "\n",
      "Trying: UMAP min_dist=0.2, HDBSCAN min_cluster_size=8\n",
      "ARI: 0.0480, Jaccard: 0.0537\n"
     ]
    }
   ],
   "source": [
    "# Try Different BERTopic Settings\n",
    "\n",
    "# Encode labels for metrics\n",
    "le = LabelEncoder()\n",
    "encoded_labels = le.fit_transform(true_labels)\n",
    "\n",
    "# Try a few configurations\n",
    "configs = [\n",
    "    {\"min_dist\": 0.5, \"min_cluster_size\": 10},\n",
    "    {\"min_dist\": 0.3, \"min_cluster_size\": 15},\n",
    "    {\"min_dist\": 0.4, \"min_cluster_size\": 12},\n",
    "    {\"min_dist\": 0.2, \"min_cluster_size\": 8},\n",
    "]\n",
    "\n",
    "for cfg in configs:\n",
    "    print(f\"\\nTrying: UMAP min_dist={cfg['min_dist']}, HDBSCAN min_cluster_size={cfg['min_cluster_size']}\")\n",
    "    \n",
    "    umap_model = UMAP(n_neighbors=30, min_dist=cfg['min_dist'], metric='cosine', random_state=42)\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=cfg['min_cluster_size'], min_samples=1, prediction_data=True)\n",
    "    \n",
    "    topic_model = BERTopic(embedding_model=embedding_model,\n",
    "                           umap_model=umap_model,\n",
    "                           hdbscan_model=hdbscan_model,\n",
    "                           calculate_probabilities=True,\n",
    "                           verbose=False)\n",
    "    \n",
    "    topics, probs = topic_model.fit_transform(texts, embeddings)\n",
    "\n",
    "    # Map -1 (unassigned) to a new label to avoid metric issues\n",
    "    topics_fixed = [t if t != -1 else max(topics) + 1 for t in topics]\n",
    "\n",
    "    # Evaluate\n",
    "    try:\n",
    "        ari = adjusted_rand_score(encoded_labels, topics_fixed)\n",
    "        jac = jaccard_score(encoded_labels, topics_fixed, average='macro')\n",
    "        print(f\"ARI: {ari:.4f}, Jaccard: {jac:.4f}\")\n",
    "    except:\n",
    "        print(\"Jaccard score could not be computed (likely due to label mismatch)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretion:\n",
    "\n",
    "**Even after hyperparameter tuning, the performance of BERTopic for aligning with manually defined aspect labels remains weak.**\n",
    "\n",
    "- The highest Adjusted Rand Index (ARI) achieved was just 0.0685, and Jaccard Coefficient peaked at 0.1735 both very low scores, indicating poor overlap and agreement between the predicted clusters and the true human-labeled aspects. For comparison, the baseline run without tuning had even worse results (e.g., ARI = -0.0386, Jaccard = 0.0101), confirming that the default clustering didn't represent meaningful service categories either.\n",
    "\n",
    "-  This outcome highlights the inherent limitations of unsupervised topic modeling for fine-grained aspect detection. Many clusters were found to be noisy or mixed, grouping unrelated topics (e.g., comments about both food and staff in the same cluster).\n",
    "\n",
    "-  These results confirm that while BERTopic can surface general themes, it lacks the precision required for aspect-level sentiment analysis compared to supervised models, rule-based lexicons, or targeted aspect extraction techniques.\n",
    "\n",
    "##### Better Alternatives for Aspect Detection:\n",
    "\n",
    "- **Aspect Term Extraction Models:** Use pretrained transformers or fine-tuned sequence labeling models (like BERT+CRF or spaCy pipelines) to extract specific nouns or noun phrases tied to sentiment.\n",
    "- **Multi-label Classification:** Train classifiers to assign multiple aspects per review using supervised labels.\n",
    "- **Manual Rules or Lexicons:** Define keyword-based heuristics for mapping sentences to known hotel service aspects (e.g., match “breakfast”, “buffet” → food).\n",
    "\n",
    "#### Part C- Aspect-wise Sentiment Classification\n",
    "\n",
    "After grouping reviews into distinct topics (aspects) using BERTopic, separate sentiment classifiers were trained for each aspect. The idea is to evaluate sentiment polarity (positive/negative) specifically within the context of each identified aspect, rather than across the whole dataset.\n",
    "\n",
    "For each aspect:\n",
    "- Data Filtering – isolate all reviews assigned to that aspect and extract their cleaned text along with sentiment labels.\n",
    "- Data Validation – skip aspects with fewer than 20 samples or with only one sentiment class, since these would not allow for meaningful training and evaluation.\n",
    "- Train-Test Split – The remaining data is split into training and test sets using stratified sampling to preserve class balance.\n",
    "- Model Pipeline – build a classification pipeline using Glove vectorization to transform the text into numerical features, followed by a Support Vector Machine to predict sentiment.\n",
    "- Evaluation – The model is trained per aspect, tested on unseen data, and evaluated using precision, recall, F1-score, and accuracy for each sentiment class.\n",
    "\n",
    "This process tells how well sentiment can be classified for each topic, revealing strengths and weaknesses in predicting sentiment at the aspect level.\n",
    "\n",
    "The aspect-based sentiment classifiers were trained using clusters generated by BERTopic as pseudo ground truth aspect labels. Each cluster was treated as an aspect, and sentiment classification was performed within each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T12:44:16.586816Z",
     "iopub.status.busy": "2025-08-23T12:44:16.586538Z",
     "iopub.status.idle": "2025-08-23T12:44:39.464888Z",
     "shell.execute_reply": "2025-08-23T12:44:39.464145Z",
     "shell.execute_reply.started": "2025-08-23T12:44:16.586788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect 20 Sentiment Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.25      0.40         8\n",
      "           1       0.80      1.00      0.89        24\n",
      "\n",
      "    accuracy                           0.81        32\n",
      "   macro avg       0.90      0.62      0.64        32\n",
      "weighted avg       0.85      0.81      0.77        32\n",
      "\n",
      "Aspect 14 Sentiment Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.55      0.55        11\n",
      "           1       0.62      0.62      0.62        13\n",
      "\n",
      "    accuracy                           0.58        24\n",
      "   macro avg       0.58      0.58      0.58        24\n",
      "weighted avg       0.58      0.58      0.58        24\n",
      "\n",
      "Aspect 6 Sentiment Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.50      0.63        26\n",
      "           1       0.81      0.97      0.88        59\n",
      "\n",
      "    accuracy                           0.82        85\n",
      "   macro avg       0.84      0.73      0.76        85\n",
      "weighted avg       0.83      0.82      0.81        85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe embeddings \n",
    "def load_glove(path):\n",
    "    glove = {}\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            parts = line.split()\n",
    "            word = parts[0]\n",
    "            vec = np.array(parts[1:], dtype='float32')\n",
    "            glove[word] = vec\n",
    "    return glove\n",
    "\n",
    "# Define Sentence Embedding Function\n",
    "def sentence_to_glove(sentence, glove_embeddings, dim=100):\n",
    "    words = sentence.split()\n",
    "    valid_vectors = [glove_embeddings[word] for word in words if word in glove_embeddings]\n",
    "    if not valid_vectors:\n",
    "        return np.zeros(dim)\n",
    "    return np.mean(valid_vectors, axis=0)\n",
    "\n",
    "# Load GloVe \n",
    "glove_path = '/kaggle/input/glove-300/glove.6B.300d.txt'\n",
    "glove = load_glove(glove_path)\n",
    "embedding_dim = 100\n",
    "\n",
    "# Loop through each aspect\n",
    "for aspect in processed_df['topic'].unique():\n",
    "    aspect_data = processed_df[processed_df['topic'] == aspect]\n",
    "    \n",
    "    if len(aspect_data) < 20:\n",
    "        continue\n",
    "\n",
    "    y = aspect_data['label'].values\n",
    "    if len(set(y)) < 2:\n",
    "        continue\n",
    "\n",
    "    # Convert text to GloVe embeddings\n",
    "    X = np.array([sentence_to_glove(text, glove, embedding_dim) for text in aspect_data['basic_text_cleaning']])\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    if not all(len(set(y[val_idx])) > 1 for _, val_idx in skf.split(X, y)):\n",
    "        continue\n",
    "\n",
    "    model = LinearSVC(max_iter=1000)\n",
    "\n",
    "    try:\n",
    "        preds = cross_val_predict(model, X, y, cv=skf)\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    print(f\"Aspect {aspect} Sentiment Classification Report:\")\n",
    "    print(classification_report(y, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Insights:\n",
    "\n",
    "- Imbalanced Classes: Most aspects show strong performance on positive sentiment and weak on negative due to data imbalance.\n",
    "- Small Sample Sizes: Aspect 14 shows the limitations of training on very small subsets — unstable and low-performing.\n",
    "- Classifier Bias: Logistic Regression tends to favor the majority class unless balanced techniques (like class weighting or resampling) are applied.\n",
    "- Overall Viability: For aspects with sufficient and balanced data (like Aspect 6), sentiment classification can be moderately effective.\n",
    "\n",
    "##### Recommendations:\n",
    "\n",
    "- Resample negative examples or undersample positives to balance.\n",
    "- Adjust class weights\n",
    "- Collect or add more negative reviews for those aspects.\n",
    "\n",
    "##### Summary:\n",
    "\n",
    "- Despite these limitations, the approach demonstrates a valid pipeline of using topic modeling for aspect detection and training aspect-specific sentiment classifiers.\n",
    "\n",
    "**The results illustrate typical real-world challenges where ground truth aspect labels are not available, and unsupervised methods must suffice.**\n",
    "\n",
    "**Future improvements could include integrating semi-supervised methods, refining cluster quality with domain knowledge, or leveraging multi-label classification frameworks to better handle overlapping aspects.**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7922761,
     "sourceId": 12548341,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8124027,
     "sourceId": 12844794,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8124182,
     "sourceId": 12845024,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
